---
title: Hierarchical Model analyses to crossed-hands data
author: "Kaian Unwalla"
date: "November, 2021"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

This code uses a Bayesian maximum likelihood estimation to estimate the internal and external weights for each participant, and also provide an estimate for the population internal and external weights and the standard deviation of the population. The model works by finding the most likely slope for each psychometric curve. 
Based on the prevailing theory for the crossed-hands deficit the model calculates the slopes for each condition as following:
- For the uncrossed curves: the slope is estimated as internal weight + external weight
- For the crossed curves: the slope is estimated as internal weight - external weight
This results in a single internal weight and external weight that is estimating the slope for both the crossed and uncrossed curves. 
- For more detail see: Unwalla, K., Goldreich, D., & Shore, D. I. (2021). Exploring reference frame integration using response demands in a tactile temporal-order judgement task. Multisensory Research, 34(8), 807-838.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.align = 'center', fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE, include = TRUE)
```

```{r, libraries}
rm(list=ls())
library(tidyverse)
library(tidyfst)
library(truncnorm)
#devtools::source_gist("524eade46135f6348140") #for stat_smooth_func (calculating slope of regression line)

source("Model functions.R")


time<-format(Sys.time(),"%b%d_%H%M%S")

path = 'Data/'
alldata1 <- read.table(paste(path,'Test Data.txt', sep = ""), header = T)

saveData = FALSE
#PPMC is a posterior predictive model check, to ensure that the model output is able to accurately estimate the raw data. NOTE: the PPMC is a slow calculation
runPPMC = FALSE
```

```{r, Hierarchical_model}
# Set variables
numIterations = 5 #number of repetitions of the MCMC chain
numMCMC = 25000 #number of iterations in the MCMC chain.  
burnIn = 5000 #amount to be removed 

#proposal distribution standard deviation parameters
wsd <- 0.14 #proposed distribution for participant weights
musd <- 0.14 #proposed distribution for population weights
sigma_sd <- 0.06 #population sigma proposal distribution
task_sd <- 0.01 #proposal distribution for the task parameter


#summarize the data
pivot_full<-alldata1 %>% 
  group_by(SOA,Condition,Hands,Participant) %>%
  summarise(numTrials = length(Actual), numRF = sum(Actual), Data = mean(Actual)) %>% 
  ungroup()

#determines number of levels of various experiment factors
numParticipants = length(unique(alldata1$Participant))
numSOA = length(unique(alldata1$SOA))
numConditions = length(unique(alldata1$Condition))
numPostures = length(unique(alldata1$Hands))
numTrialsCondition = nrow(alldata1)/numParticipants/numSOA/numConditions/numPostures

SOA <- unique(pivot_full$SOA) #levels of SOAs for hypothesis
PCD_easy <- pivot_full %>% select(-c(numRF,numTrials,Data)) #for PPMC


#create empty lists to later save with overall outputted data
popParamsAll <- list()
trialInfoAll <- list()
wIntAll <- list()
wExtAll <- list()


for(i in 1:numIterations){
  
  #emtpy lists to save chosen parameters on each trial
  saved_popHyps <-list()
  saved_trial <- list()
  saved_wInt <- list()
  saved_wExt <- list()
  saved_PPMC <- list()
  
  # Generate initial hypothesis
  #set initial parameters for MCMC. This needs to be inside the first for loop so that each MCMC chain is initialized to a different random variable
  minWeight = 2
  maxWeight = 20
  minSD = 1
  maxSD = 8
  minTask = 0.5
  maxTask = 2.5
  wInt_current <- c(P = runif(numParticipants,minWeight,maxWeight))
  wExt_current <- c(P = runif(numParticipants,minWeight,maxWeight))
  
  hyp_current_pop <- c(Internal = runif(1,minWeight,maxWeight),
                       External = runif(1,minWeight,maxWeight),
                       sdInt =runif(1,minSD,maxSD),
                       sdExt = runif(1,minSD,maxSD),
                       taskInt = runif(1,minTask,maxTask),
                       taskExt = runif(1,minTask,maxTask)) 
  
  #calculating prior probability using a truncated normal distribution for the participants, 
  #and uniform prior for population weights
  prior_current <- sum(log(dtruncnorm(wInt_current, 0, Inf, hyp_current_pop[1], hyp_current_pop[3])),
                       log(dtruncnorm(wExt_current, 0, Inf, hyp_current_pop[2], hyp_current_pop[4])),
                       log(ifelse(dunif(hyp_current_pop[1], 0,100) > 0, 1, 0)), #population internal
                       log(ifelse(dunif(hyp_current_pop[2], 0,100) > 0, 1, 0)), #population external  
                       log(ifelse(dunif(hyp_current_pop[5], 0,10) > 0, 1, 0)), #internal task parameter
                       log(ifelse(dunif(hyp_current_pop[6], 0,10) > 0, 1, 0))) #external task parameter 
  
  
  #calculates hypothesized probabilities
  hypothesis_current <- genHypothesis(wInt_current, wExt_current, 
                                      SOA, 
                                      hyp_current_pop[5], hyp_current_pop[6], 
                                      numTrialsCondition)
  
  #calculates likelihood of the hypothesis
  likelihood_current <- sum(compLikelihood(hypothesis_current, pivot_full$numTrials, pivot_full$numRF))

  #calculates numerator
  numerator_current <- prior_current + likelihood_current
  
  
  #Comparison point
  for(count in 1:numMCMC){
    
    #proposed parameters for MCMC
    wInt_propose <- c(P = rnorm(numParticipants, mean = wInt_current, sd = wsd))
    wExt_propose <- c(P = rnorm(numParticipants, mean = wExt_current, sd = wsd))
    
    hyp_propose_pop <- c(Internal = rnorm(1,mean=hyp_current_pop[1],sd=musd),
                         External = rnorm(1,mean=hyp_current_pop[2],sd=musd),
                         sdInt =rnorm(1,mean=hyp_current_pop[3],sd=sigma_sd),
                         sdExt = rnorm(1,mean=hyp_current_pop[4],sd=sigma_sd),
                         taskInt = rnorm(1,mean=hyp_current_pop[5],sd=task_sd),
                         taskExt = rnorm(1,mean=hyp_current_pop[6],sd=task_sd))
    
    #calculating proposed numerator same as initial hypothesis
    prior_propose <- sum(log(dtruncnorm(wInt_propose, 0, Inf, hyp_propose_pop[1], hyp_propose_pop[3])),
                         log(dtruncnorm(wExt_propose, 0, Inf, hyp_propose_pop[2], hyp_propose_pop[4])),
                         log(ifelse(dunif(hyp_propose_pop[1], 0,100) > 0, 1, 0)),
                         log(ifelse(dunif(hyp_propose_pop[2], 0,100) > 0, 1, 0)),                       
                         log(ifelse(dunif(hyp_propose_pop[5], 0,10) > 0, 1, 0)),
                         log(ifelse(dunif(hyp_propose_pop[6], 0,10) > 0, 1, 0))) 
    
    hypothesis_propose <-genHypothesis(wInt_propose, wExt_propose,
                                       SOA,
                                       hyp_propose_pop[5], hyp_propose_pop[6], 
                                       numTrialsCondition)
    
    likelihood_propose <- sum(compLikelihood(hypothesis_propose, pivot_full$numTrials, pivot_full$numRF))
    
    numerator_propose <- prior_propose + likelihood_propose
    
    
    #comparing hypotheses
    ratio = exp(numerator_propose - numerator_current)
    
    probability <- ifelse(ratio > 1, 1, rbinom(1,1,ratio)) 
    #if the p(comparison) > p(original) then  probability of jumping = 1
    #if p(comparison) < p(original) then jump with probably = ratio
    #so, jump if ratio > 1, OR if coin flip favours a jump
    if(probability){
      #overwrites all the current lists with the proposed lists
      wInt_current <- wInt_propose
      wExt_current <- wExt_propose
      hyp_current_pop <- hyp_propose_pop
      prior_current <- prior_propose
      hypothesis_current <- hypothesis_propose
      likelihood_current <- likelihood_propose
      numerator_current <- numerator_propose
      result = 'Jump'
    } else{
      result <- 'Stay'
    }
    
    #PPMC
    #run only on the final MCMC run, and if runPPMC is set to TRUE
    if(runPPMC && i == numIterations){
      allPPMC<-PPMC(PCD_easy, pRfirst)
      saved_PPMC[[count]] <- allPPMC
    }
    
    #Putting all the trial data in 1 list
    trial_info <- c(prior = prior_current, likelihood = likelihood_current, numerator = numerator_current, result = result)
    
    #adds parameters of the more probable hypothesis on each trial to list
    saved_popHyps[[count]] <- hyp_current_pop
    saved_trial[[count]] <- trial_info
    saved_wInt[[count]] <- wInt_current
    saved_wExt[[count]] <- wExt_current
    
  }
  
  #Converting saved data lists to tibble
  saved_popHyps <- do.call(bind_rows, saved_popHyps) %>% 
    mutate(Iteration = i, Trial = 1:numMCMC)
  saved_trial <- do.call(bind_rows, saved_trial) %>% 
    mutate(Iteration = i, Trial = 1:numMCMC)
  saved_wInt <- do.call(bind_rows, saved_wInt) %>%
    mutate(Iteration = i, Trial = 1:numMCMC)
  saved_wExt <- do.call(bind_rows, saved_wExt) %>%
    mutate(Iteration = i, Trial = 1:numMCMC)
  
  #Saving each iteration as a list
  popParamsAll[[i]] <- saved_popHyps
  trialInfoAll[[i]] <- saved_trial
  wIntAll[[i]] <- saved_wInt
  wExtAll[[i]] <- saved_wExt
  
}


#Converting saved data of each iteration to tibble
popParamsAll <- do.call(bind_rows,popParamsAll) 
trialInfoAll <- do.call(bind_rows, trialInfoAll)
wIntAll <- do.call(bind_rows, wIntAll) %>%
  mutate(RF = "Internal")
wExtAll <- do.call(bind_rows, wExtAll) %>%
  mutate(RF = "External")

#joining participant internal and external weights together
partParamsAll <- rbind(wIntAll,wExtAll) 
colNames <- c(1:numParticipants,"Iteration","Trial","RF")
names(partParamsAll)<-colNames
partParamsAll <- partParamsAll %>% 
  pivot_longer(names_to = "Participant", values_to = "A", cols = 1:numParticipants)

if(saveData){
  saveRDS(popParamsAll, paste(path,"Population Parameters.rds",sep = ""), compress = "xz")
  saveRDS(trialInfoAll, paste(path,"Trial Info.rds", sep = ""), compress = "xz")
  saveRDS(partParamsAll, paste(path,"Participant Parameters.rds", sep = ""), compress = "xz")
}

if(runPPMC && i == numIterations){
  PPMC <- do.call(bind_rows, saved_PPMC) %>% 
    mutate(Iteration = i, Trial = 1:numMCMC)
  if(saveData){
    saveRDS(PPMC, paste(path,"PPMC.rds", sep = ""), compress = "xz")
  }
}
#This calculates the number of times the MCMC chain jumped to a new hypothesis. Good acceptance rates vary based on the number of parameters. 
Acceptance <- trialInfoAll %>% 
  filter(Trial > burnIn) %>% 
  group_by(Iteration) %>% 
  mutate(acceptance = ifelse(result == 'Jump', 1, 0)) %>% 
  summarise(Acceptance = round(mean(acceptance),2)) 
AR = c(Acceptance$Acceptance)


runDetails<-file(paste(path, "run details.txt", sep=""))
writeLines(c(paste("Hierarchical Model"),
             "",
             time,
             "",
             paste("Total Number of Iterations:", numIterations),
             paste("Trials per Iteration:", numMCMC),
             paste("Burn In:", burnIn),
             "",
             "Parameter Initialization Range", 
             paste("participant weights:", minWeight, "-", maxWeight), 
             paste("population weights:", minWeight, "-", maxWeight), 
             paste("population sigma:", minSD, "-", maxSD), 
             paste("task parameter:", minTask,"-", maxTask),
             "",
             "Proposal Distribution Sigmas", 
             paste("participant weights:", wsd), 
             paste("population weights:", musd), 
             paste("population sigma:", sigma_sd), 
             paste("task parameter:", task_sd),
             "",
             paste("Acceptance Rate (after burn-in):", AR)
), runDetails)
close(runDetails)




```

```{r, load_data}
#loads in the data if Hierarchical model previously completed
alldata1 <- read.table(paste(path,'Test Data.txt', sep = ""), header = T)
popParamsAll <- readRDS(paste(path,'Population Parameters.rds', sep = "")) %>% filter(Trial > burnIn) %>% 
  mutate(Participant = as.integer(Participant))
partParamsAll <- readRDS(paste(path,'Participant Parameters.rds', sep = "")) %>% filter(Trial > burnIn)
trialInfoAll <- readRDS(paste(path,'Trial Info.rds', sep = "")) %>% filter(Trial > burnIn)
if(runPPMC){
  readRDS(paste(path,'PPMC.rds', sep = "")) %>% filter(Trial > burnIn)
}
```

```{r, organizing_data}
popParamsAll <- popParamsAll %>% filter(Trial > burnIn)
partParamsAll <- partParamsAll %>% filter(Trial > burnIn) %>% 
    mutate(Participant = as.integer(Participant))
trialInfoAll <- trialInfoAll %>% filter(Trial > burnIn)
if(runPPMC){
  PPMC <- PPMC %>% filter(Trial > burnIn)
}

#calculates population weights for each condition
popParams_Condition <- rbind(popParamsAll %>% mutate(Condition = "A"), 
                             popParamsAll %>% mutate(Condition = "B")) %>% #duplicate data frame with added column for condition
  mutate(Internal = ifelse(Condition == "B", Internal*taskInt, Internal),
         External = ifelse(Condition == "B", External*taskExt, External)) #multiply weights for condition B by the task parameters

#calculates posterior means for participants
participant_weights <- full_join(popParamsAll, partParamsAll) %>% 
  mutate(B = ifelse(RF == "Internal", A*taskInt, A*taskExt)) %>% 
  group_by(Participant,RF) %>% 
  summarise(A = mean(A), B = mean(B)) %>% 
  pivot_longer(names_to = "Condition", values_to = "Weight", cols = c(A,B)) %>% 
  pivot_wider(names_from = RF, values_from = Weight) 


```

## Confidence Intervals

This code calculates the mean value and 95 percent confidence intervals for each of the population parameters (weights and standard deviations). By looking at whether the confidence intervals surrounding the task parameters contains 0, will determine whether the parameter significantly changed for each condition. 
```{r 95_CI}
CI <- popParams_Condition %>%  
  pivot_longer(names_to = "RF", values_to = "Weight", cols = c(Internal,External)) %>% 
  group_by(Iteration,Condition,RF) %>% 
  summarise(lowCI = quantile(Weight, 0.025, type = 6),
            highCI = quantile(Weight, 0.975, type = 6),
            Weight = mean(Weight)) %>% 
  group_by(Condition,RF) %>% 
  summarise(PopWeight = mean(Weight), lowCI = mean(lowCI), highCI = mean(highCI))

CI_taskParam <- popParamsAll %>% 
  pivot_longer(names_to = "RF", values_to = "taskParam", cols = c(taskInt,taskExt)) %>% 
  group_by(Iteration,RF) %>% 
  summarise(lowCI = quantile(taskParam, 0.025, type = 6),
            highCI = quantile(taskParam, 0.975, type = 6),
            taskParam = mean(taskParam)) %>% 
  group_by(RF) %>% 
  summarise(taskParam = mean(taskParam), lowCI = mean(lowCI), highCI = mean(highCI))

CI_sd <- popParamsAll %>% 
  pivot_longer(names_to = "RF", values_to = "SD", cols = c(sdInt,sdExt)) %>% 
  group_by(Iteration,RF) %>% 
  summarise(lowCI = quantile(SD, 0.025, type = 6),
            highCI = quantile(SD, 0.975, type = 6),
            SD = mean(SD)) %>% 
  group_by(RF) %>% 
  summarise(SD = mean(SD), lowCI = mean(lowCI), highCI = mean(highCI))
```

## Visualizations

This provides a visualization of how the weights change in response to the changed condition for each participant
```{r MCMC_participant_parameters}
participant_weights %>% 
  ggplot(aes(x=Internal, y=External, fill=Condition))+
  geom_point(size = 3, shape=21)+
  geom_path(arrow=arrow(type = 'closed',length = unit(0.3, "cm"), ends = 'last'), 
            size = 0.5, alpha = 0.3, aes(group=Participant))+
  scale_fill_manual(values = c('black','blue'))+
  geom_abline(slope=1,intercept=0, linetype = 'dashed')+
  annotate(geom = 'text', label = 'A', x=-Inf, y=Inf, hjust = -2, vjust=2, size=7, fontface = 'bold')+
  ylab('External Weight')+xlab("Internal Weight")

```

Here we are comparing the weights predicted by the hierarchical model to the true weights used to create the data. This allows us to get a better understanding on whether the model is settling near the true weights (a sign the model works). 
```{r, real_vs_estimated_weights}
#summarize the true weights of each participant
trueWeights <- alldata1 %>% 
  group_by(Participant,Condition) %>% 
  summarize(Internal = mean(wInt), External = mean(wExt), Type = 'True') #setting a dummy likelihood to easily group the data in the graph

rbind(trueWeights, participant_weights %>% mutate(Type = 'MCMC')) %>% 
  ggplot(aes(x = Internal, y = External))+
  geom_path(aes(group = Participant), color = 'grey')+
  geom_point(aes(color = as.character(Type)), size = 2)+
  xlab("Internal Weight")+ylab("External Weight")+
  scale_color_manual(values = c('black','grey'), labels = c("Estimated Weight", "True Weight"))+
  facet_wrap(~Condition)
```
- The model does a decent job estimating the participant weights, but the model estimated the weights more accurately in condition A. 
- This likely the result of the requirement to find a slope that works for both the crossed and uncrossed curves, and as performance improves it provides many more weights that can accurately fit the data. 


Now lets look at the task parameters predicted by the hierarchical model to the true task parameters used to create the data. This allows us to get a better understanding on whether the model is settling near the true parameters (a sign the model works).
```{r, real_vs_estimated_weights}
#summarize the true weights of each participant
trueTaskparam <- alldata1 %>% 
  summarize(Internal = mean(taskInt), External = mean(taskExt), Type = 'True') #setting a dummy likelihood to easily group the data in the graph

rbind(trueTaskparam, popParamsAll %>% summarize(Internal = mean(taskInt), External = mean(taskExt), Type = 'Hierarchical')) %>% 
  pivot_longer(names_to = "RF", values_to = "Weight", cols = c(Internal,External)) %>% 
  ggplot(aes(x = RF, y = Weight, fill = Type))+
  geom_bar(stat = "identity", position = position_dodge(), size = 1)+
  xlab("Reference Frame")+ylab("Task Parameter Value")+
  scale_fill_manual(values = c('black','grey'))
```
- The model predicts the both task parameters almost perfectly. This suggests the model is accurately capturing the true data. 

Another way to see how similar the model output is to the true data, is to look at a measure that wasn't directly measured by the model. Here I have used the PCD score. By calculating the PCD score based on the estimated data and comparing it to the true data we can see how well the model output maps onto the data. 
```{r, MCMC_actual_vs_predictedPCD, width = 18, height = 7}
#this calculates a PCD score based on the predicted data and summarizes the participants actual PCD score
modelPCD <- full_join(Pivot(alldata1), participant_weights) %>% 
  mutate(Estimated = genHypothesisMLE(Internal,External,Hands,SOA,Condition)) %>% 
  gather(Type, pRfirst, Data,Estimated) %>% 
  select(-numTrials,-numRF) %>% 
  spread(key= Hands, pRfirst) %>% 
  group_by(Type,Participant,Condition) %>% 
  mutate(PCD = ifelse(SOA < 0, (Crossed - Uncrossed),(Uncrossed - Crossed))) %>% 
  summarize(PCD = sum(PCD)) %>% 
  ungroup() %>% 
  spread(key=Type,PCD)


#plots participants actual PCD score with their predicted PCD score
modelPCD %>% 
  ggplot(aes(x=Data, y=Estimated))+
  geom_point(size = 3, shape = 21, aes(fill = Condition))+
  scale_fill_manual(values = c('black','blue'))+
  xlim(0,5.5)+ylim(0,5.5)+
  geom_abline(slope = 1, intercept = 0)+
  ylab('Estimated PCD')+xlab("True PCD")
```
- Here we have the true PCD score on the x-axis and the model estimated PCD on the y-axis. I have plotted a straight line with a slope of 1, going through the origin. The closer the points fall to this line, the more similar the model output to the data. We can see that the model is actually doing a good job at estimating the PCD scores. 

Typically, we would not be able to validate our model by looking at the true weights. In order to validate a model we would conduct a posterior predictive model check. This simulates data under the new model, and compares this with our observed data. I conducted the PPMC on the PCD score so that we can easily compare the simulated score with our observed data. 
```{r, PPMC, width = 15, height = 7}
# This calculates the PCD and correlation value to plot on PPMC graph
PCD_overall<-PCD(alldata1) %>%
  group_by(Condition) %>%
  summarize(PCD = mean(PCD))

mean_PCD = c(PCD_overall$PCD)

PPMC %>% 
  pivot_longer(names_to = "PCD", values_to = "value", cols = c(A:B)) %>% 
  ggplot(aes(x=value, y=..count../sum(count), fill=PCD))+
  ylab("Probability")+xlab("PCD Score")+
  #xlim(0,2)+
  geom_histogram(binwidth = 0.05, position = 'identity', color='black', alpha=0.8)+
  scale_fill_manual(values=c('black','blue'))+
  geom_vline(xintercept = mean_PCD, color = c('black','blue'), linetype = 'dashed')+
  theme(legend.position = c(0, 1), legend.justification = c('left','top'), legend.key.width = unit(0.5,'cm'))
```
- We can see that the average PCD score outputted by the model matches very closely with the observed PCD score. This suggests that the model is doing a very good job estimating the parameters of our observed data. 


